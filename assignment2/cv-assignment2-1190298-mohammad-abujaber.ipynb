{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Birzeit University\n","\n","### Department of Electrical & Computer Engineering\n","\n","### First Semester, 2023/2024\n","\n","### ENCS5343 Computer Vision\n","\n","### Assignment#2 CBIR Ststem\n","\n","### Mohammad AbuJaber - 1190298\n"]},{"cell_type":"markdown","metadata":{},"source":["# Introduction\n"]},{"cell_type":"markdown","metadata":{},"source":["Content-Based Image Retrieval (CBIR) is a computer vision and information retrieval technique that uses visual features of images to search and retrieve them from a database. Unlike traditional methods that rely on metadata or textual descriptions, CBIR focuses on the visual features of images, such as color, texture, shape, and spatial arrangements. CBIR systems use an image query to search for similar images in a database by comparing the visual content of the query image with the features extracted from the stored images. This involves feature extraction techniques to represent visual content numerically and similarity measures to quantify the resemblance between images. CBIR has various applications, including medical imaging, multimedia retrieval, e-commerce, and law enforcement. It offers advantages such as no need for keywords, more accurate results, faster searching, and no subjective interpretation. However, CBIR requires access to a large and well-indexed database of images and can be computationally expensive. Despite these limitations, CBIR is a powerful technology that is constantly evolving, making it an essential tool for anyone working with images.\n"]},{"cell_type":"markdown","metadata":{},"source":["Color histograms and color moments are two essential tools for analyzing and summarizing the color distribution within an image. Both are based on probability theory and offer complementary insights for applications like image retrieval, segmentation, and classification.\n","\n","### Color histogram\n","\n","Color histograms are graphical representations of the distribution of colors within an image, dividing the color space into discrete bins and counting the number of pixels falling into each bin. They are simple to compute and visually intuitive, but are sensitive to illumination changes and prone to quantization errors.\n","\n","### Color moments\n","\n","Color moments are statistical measures that capture properties of the color distribution in an image, calculated from the mean, variance, and higher-order moments of the pixel color intensities. They are based on statistical moment theory and provide quantitative descriptors of the image's color distribution. They are robust to illumination changes, insensitive to quantization errors, and capture spatial information through higher-order moments.\n","\n","The choice between color histograms and color moments depends on the specific application and desired information. Histograms are efficient for gross color trends and quick comparisons, while moments offer deeper statistical insights and robustness to lighting changes. Combining both approaches can provide a comprehensive understanding of an image's color content.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset\n","\n","A modified version of [\"corel-images\"](https://www.kaggle.com/datasets/elkamel/corel-images) datasert consists of two main directories, \"horses_test\" and \"training_set,\" organized into subdirectories corresponding to various image categories. The \"horses_test\" directory contains images specifically chosen for testing or querying the Content-Based Image Retrieval (CBIR) system. The \"training_set\" directory includes images from 10 distinct categories, such as beaches, buses, dinosaurs, elephants, flowers, foods, horses, monuments, mountains_and_snow, and people_and_villages_in_Africa. Each subdirectory contains images relevant to its category, forming a comprehensive training set for the CBIR system. This variety enables the CBIR system to learn and recognize diverse visual features associated with different image categories, contributing to its ability to retrieve visually similar images based on color histograms.\n","\n","For the modified versoion of dataset [\"click here\"](https://drive.google.com/drive/folders/1F7wiYMAxT43enCSYc3THVgDjLoIAeoOD?usp=sharing).\n"]},{"cell_type":"markdown","metadata":{},"source":["> ### **Task 1:**\n",">\n","> Build the CBIR system: Design and implement a system architecture for image retrieval using color features. Develop functionalities for loading images, extracting features, computing distances, and ranking results.\n"]},{"cell_type":"markdown","metadata":{},"source":["The following code shows an example implementation of CBIR System.\n"]},{"cell_type":"markdown","metadata":{},"source":["### System Setup (CBIRSystem Class)\n","\n","- **`__init__` (Initialization):** The class takes a dataset path as input and initializes empty lists for image paths and features.\n","- **`load_images`:** This method traverses the dataset path and collects paths of images with specific file extensions (png, jpg, jpeg).\n","- **`extract_features`:** For each image in the dataset, it extracts color histograms using the calculate_histogram method and flattens them for feature representation.\n","- **`calculate_histogram`:** This method computes a 3D color histogram for an image using the OpenCV function cv2.calcHist. The histogram is then normalized.\n","- **`search_similar_images`:** Given a query image path, this method calculates the color histogram for the query image and compares it with the histograms of all images in the dataset using Euclidean distance. The top-k similar images are returned based on the smallest Euclidean distances.\n","- **`plot_results`:** This method visualizes the query image and the top-k similar images in a 2x5 grid using Matplotlib.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-01-07T17:01:48.180572Z","iopub.status.busy":"2024-01-07T17:01:48.180144Z","iopub.status.idle":"2024-01-07T17:01:48.199915Z","shell.execute_reply":"2024-01-07T17:01:48.198627Z","shell.execute_reply.started":"2024-01-07T17:01:48.180541Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.metrics.pairwise import euclidean_distances\n","import matplotlib.pyplot as plt\n","\n","\n","class CBIRSystem:\n","    def __init__(self, dataset_path):\n","        self.dataset_path = dataset_path\n","        self.image_paths = []\n","        self.features = []\n","\n","    def load_images(self):\n","        for root, dirs, files in os.walk(self.dataset_path):\n","            for file in files:\n","                if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    image_path = os.path.join(root, file)\n","                    self.image_paths.append(image_path)\n","\n","    def extract_features(self):\n","        for image_path in self.image_paths:\n","            img = cv2.imread(image_path)\n","            hist = self.calculate_histogram(img)\n","            self.features.append(hist.flatten())\n","\n","    def calculate_histogram(self, image):\n","        hist = cv2.calcHist(\n","            [image], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]\n","        )\n","        cv2.normalize(hist, hist)\n","        return hist\n","\n","    def search_similar_images(self, query_image_path, top_k=10):\n","        query_img = cv2.imread(query_image_path)\n","        query_hist = self.calculate_histogram(query_img).flatten()\n","\n","        distances = euclidean_distances([query_hist], self.features).flatten()\n","        sorted_indices = np.argsort(distances)[:top_k]\n","\n","        return [\n","            (self.image_paths[i], os.path.basename(self.image_paths[i]))\n","            for i in sorted_indices\n","        ]\n","\n","    def plot_results(self, query_image_path, result_image_info):\n","        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","\n","        # Plot query image\n","        query_img = cv2.imread(query_image_path)\n","        axes[0, 0].imshow(cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB))\n","        axes[0, 0].set_title(\"Query Image\")\n","        axes[0, 0].axis(\"off\")\n","\n","        # Plot similar images\n","        for i, (result_image_path, result_image_name) in enumerate(result_image_info):\n","            img = cv2.imread(result_image_path)\n","            axes[i // 5, i % 5].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","            axes[i // 5, i % 5].set_title(result_image_name)\n","            axes[i // 5, i % 5].axis(\"off\")\n","\n","        plt.tight_layout()\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Usage Example\n","\n","- **Dataset and Query Image:**\n","\n","Load images from a dataset path (`/kaggle/input/corel-images/dataset/training_set`). Then, specify a query image path such as (`/kaggle/input/corel-images/dataset/horses_test/700.jpg`).\n","\n","- **Image Loading and Feature Extraction:**\n","\n","Load and display the query image.\n","Load all images from the dataset and extract their color histograms.\n","\n","- **Image Retrieval:**\n","\n","Search for the top 10 most similar images to the query image based on histogram comparison.\n","\n","- **Result Visualization:**\n","\n","Display the query image and the top 10 retrieved results in a grid arrangement.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-07T17:01:48.202887Z","iopub.status.busy":"2024-01-07T17:01:48.202536Z","iopub.status.idle":"2024-01-07T17:01:51.836703Z","shell.execute_reply":"2024-01-07T17:01:51.835684Z","shell.execute_reply.started":"2024-01-07T17:01:48.202858Z"},"trusted":true},"outputs":[],"source":["dataset_path = \"/kaggle/input/corel-images/dataset/training_set\"\n","cbir_system = CBIRSystem(dataset_path)\n","cbir_system.load_images()\n","cbir_system.extract_features()\n","print(\"Number of images in the dataset: \", len(cbir_system.image_paths))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-07T17:01:51.838479Z","iopub.status.busy":"2024-01-07T17:01:51.838025Z","iopub.status.idle":"2024-01-07T17:01:53.916763Z","shell.execute_reply":"2024-01-07T17:01:53.915652Z","shell.execute_reply.started":"2024-01-07T17:01:51.838441Z"},"trusted":true},"outputs":[],"source":["query_image_path = \"/kaggle/input/corel-images/dataset/horses_test/700.jpg\"\n","query_img = cv2.imread(query_image_path)\n","plt.imshow(cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB))\n","plt.title(\"Query Image\")\n","plt.axis(\"off\")\n","plt.show()\n","result_images = cbir_system.search_similar_images(query_image_path, top_k=10)\n","cbir_system.plot_results(query_image_path, result_images)"]},{"cell_type":"markdown","metadata":{},"source":["### Key Points\n","\n","The code uses color histograms for image features, providing a basic CBIR method. Euclidean distance is used for similarity comparison, but may not accurately capture perceptual differences. The code is modular and well-organized, and can be adapted to use different features, similarity metrics, and datasets. Potential extensions include feature exploration and exploring alternative similarity metrics.\n"]},{"cell_type":"markdown","metadata":{},"source":["> ### **Task 2:**\n",">\n","> Implement the CBIR system using Color Histogram as an image representation. Experiment with 120 bins, 180 bins and with 360 bins. Use Euclidean as distant measure and compute precision, recall, F1 score, and time for each experiment. Construct a Receiver Operating Characteristic (ROC) curve by varying the retrieval threshold. Calculate the Area Under the Curve (AUC) to measure the overall performance across different threshold settings. Note that you need to compute these measures as an average of at least 10 different quires.\n"]},{"cell_type":"markdown","metadata":{},"source":["***NOTE***: As it was told to us, the main objective of this task is to determine the effectiveness of the number of bins. So, any three different values, up to 256 bins, can be used. For simplification, I used 8 bins, 100 bins, and 150 bins."]},{"cell_type":"markdown","metadata":{},"source":["The code includes a utility function for managing feature files, measuring memory usage, and execution time. The `delete_features_file` function deletes a specified feature file, while the measure_memory_usage_and_time_spent function measures memory usage and time spent during another function's execution. These utilities are useful for optimizing data processing pipelines, especially in large datasets or resource-intensive operations. The `psutil` library allows monitoring of memory usage, while the `time` module measures execution time. The code emphasizes resource management and optimization in image processing and retrieval tasks.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:20:42.788833Z","iopub.status.busy":"2024-01-08T17:20:42.788407Z","iopub.status.idle":"2024-01-08T17:20:42.800522Z","shell.execute_reply":"2024-01-08T17:20:42.799269Z","shell.execute_reply.started":"2024-01-08T17:20:42.788798Z"},"trusted":true},"outputs":[],"source":["import os\n","import psutil\n","import time\n","\n","def delete_features_file(features_file=\"features.pkl\"):\n","    try:\n","        os.remove(features_file)\n","    except FileNotFoundError:\n","        print(f\"{features_file} not found.\")\n","\n","dataset_path = \"/kaggle/input/corel-images/dataset/training_set\"\n","query_images_path = \"/kaggle/input/corel-images/dataset/horses_test\"\n","\n","def measure_memory_usage_and_time_spent(func, *args, **kwargs):\n","    start_time = time.time()\n","    before_memory = psutil.virtual_memory().used / (1024 ** 2)\n","    result = func(*args, **kwargs)\n","    after_memory = psutil.virtual_memory().used / (1024 ** 2)\n","    memory_usage = after_memory - before_memory\n","    print(f\"Memory usage during the operation: {memory_usage:.2f} MB\")\n","    end_time = time.time()\n","    elapsed_time_seconds = end_time - start_time\n","    hours, remainder = divmod(elapsed_time_seconds, 3600)\n","    minutes, seconds = divmod(remainder, 60)\n","    time_format = \"{:02}:{:02}:{:02}\".format(int(hours), int(minutes), int(seconds))\n","    print(\"Time spent: {}\".format(time_format))\n","    return result"]},{"cell_type":"markdown","metadata":{},"source":["This code consumes a lot of memory, so I modified the basic `CBIRSystem class` to save the features extracted on a file called `features.pkl` to avoid crashes. It will load images from the dataset, extract features using a color histogram with a specific number of bins each time, then search for similar images as it will sort the images according to Euclidean distance, take the top 10 images, and plot them. The `evaluate_performance` function evaluates system performance using precision, recall, F1-score, and ROC-AUC metrics. It iterates through query images, calculates distances, computes precision, recall, and F1-score for each query, aggregates scores, creates a ROC curve, calculates AUC, and displays performance metrics. This comprehensive assessment of the CBIR system's retrieval performance is essential for analysis.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T09:40:07.501866Z","iopub.status.busy":"2024-01-08T09:40:07.501319Z","iopub.status.idle":"2024-01-08T09:40:07.548151Z","shell.execute_reply":"2024-01-08T09:40:07.546987Z","shell.execute_reply.started":"2024-01-08T09:40:07.501795Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from sklearn.metrics.pairwise import euclidean_distances\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import auc\n","import pickle\n","\n","class CBIRSystem:\n","    def __init__(self, dataset_path, features_file=\"features.pkl\"):\n","        self.dataset_path = dataset_path\n","        self.image_paths = []\n","        self.features_file = features_file\n","        self.features = None\n","\n","    def load_images(self):\n","        self.image_paths = [\n","            os.path.join(root, file)\n","            for root, dirs, files in os.walk(self.dataset_path)\n","            for file in files\n","            if file.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n","        ]\n","\n","    def extract_features(self, bin_size=8):\n","        with open(self.features_file, 'wb') as f:\n","            for image_path in self.image_paths:\n","                img = cv2.imread(image_path)\n","                r_img = cv2.resize(img, (5, 5))\n","                hist = self.calculate_histogram(r_img, bin_size)\n","                pickle.dump(hist.flatten(), f)\n","\n","    def load_features(self):\n","        with open(self.features_file, 'rb') as f:\n","            self.features = [pickle.load(f) for _ in range(len(self.image_paths))]\n","\n","    def clear_features(self):\n","        self.features = None\n","\n","    def calculate_histogram(self, image, bin_size):\n","        hist = cv2.calcHist(\n","            [image], [0, 1, 2], None, [bin_size, bin_size, bin_size], [0, 256, 0, 256, 0, 256]\n","        )\n","        cv2.normalize(hist, hist)\n","        return hist\n","\n","    def search_similar_images(self, query_image_path, top_k=10, bin_size=8):\n","        query_hist = self.calculate_histogram(cv2.imread(query_image_path), bin_size).flatten()\n","        distances = euclidean_distances([query_hist], self.features).flatten()\n","        sorted_indices = np.argsort(distances)[:top_k]\n","        return [(self.image_paths[i], os.path.basename(self.image_paths[i])) for i in sorted_indices]\n","\n","    def plot_results(self, query_image_path, result_image_info):\n","        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","        query_img = cv2.imread(query_image_path)\n","\n","        axes[0, 0].imshow(cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB))\n","        axes[0, 0].set_title(\"Query Image\")\n","        axes[0, 0].axis(\"off\")\n","\n","        for i, (result_image_path, result_image_name) in enumerate(result_image_info):\n","            img = cv2.imread(result_image_path)\n","            axes[i // 5, i % 5].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","            axes[i // 5, i % 5].set_title(result_image_name)\n","            axes[i // 5, i % 5].axis(\"off\")\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def calculate_euclidean_distances(self, query_image_path, bin_size=8):\n","        query_hist = self.calculate_histogram(cv2.imread(query_image_path), bin_size).flatten()\n","        distances = euclidean_distances([query_hist], self.features).flatten()\n","        return distances\n","\n","    def evaluate_performance(self, query_images_path, bin_size=8):\n","        all_true_labels = []\n","        all_distances = []\n","        precision_list = []\n","        recall_list = []\n","        f1_score_list = []\n","        ground_truth_labels = [1 if 'horses' in path and '710.jpg' <= path.split(os.path.sep)[-1] <= '799.jpg' else 0 for path in self.image_paths]\n","\n","        TP_list, FP_list, TN_list, FN_list = [], [], [], []\n","\n","        for query_image in os.listdir(query_images_path):\n","            TP, FP, TN, FN = 0, 0, 0, 0\n","            query_image_path = os.path.join(query_images_path, query_image)\n","            distances = self.calculate_euclidean_distances(query_image_path, bin_size=bin_size)\n","            sorted_indices = np.argsort(distances)\n","            true_labels = ground_truth_labels\n","\n","            for i in range(10):\n","                if true_labels[sorted_indices[i]] == 1:\n","                    TP += 1\n","                else:\n","                    FP += 1\n","\n","            for i in range(10, len(self.image_paths)):\n","                if true_labels[sorted_indices[i]] == 0:\n","                    TN += 1\n","                else:\n","                    FN += 1\n","\n","            TP_list.append(TP)\n","            FP_list.append(FP)\n","            TN_list.append(TN)\n","            FN_list.append(FN)\n","\n","            all_true_labels.extend(true_labels)\n","            all_distances.extend(distances)\n","\n","            precision = TP / (TP + FP) if TP + FP != 0 else 0\n","            recall = TP / (TP + FN) if TP + FN != 0 else 0\n","            f1_score = 2 * precision * recall / (precision + recall) if precision + recall != 0 else 0\n","\n","            # Store metrics for averaging\n","            precision_list.append(precision)\n","            recall_list.append(recall)\n","            f1_score_list.append(f1_score)\n","\n","            print(\"Metrics for Query:\", query_image)\n","            print(\"Precision:\", precision)\n","            print(\"Recall:\", recall)\n","            print(\"F1 Score:\", f1_score)\n","\n","        # Calculate and print average Precision, Recall, and F1 Score\n","        avg_precision = sum(precision_list) / len(precision_list)\n","        avg_recall = sum(recall_list) / len(recall_list)\n","        avg_f1_score = sum(f1_score_list) / len(f1_score_list)\n","\n","        print(\"Average Precision:\", avg_precision)\n","        print(\"Average Recall:\", avg_recall)\n","        print(\"Average F1 Score:\", avg_f1_score)\n","        \n","\n","        sorted_indices_all = np.argsort(all_distances)\n","        total_positives, total_negatives = sum(all_true_labels), len(all_true_labels) - sum(all_true_labels)\n","\n","        fpr = [0]\n","        tpr = [0]\n","        for i in range(len(all_true_labels)):\n","            if all_true_labels[sorted_indices_all[i]] == 1:\n","                tpr.append(tpr[-1] + 1 / total_positives)\n","                fpr.append(fpr[-1])\n","            else:\n","                tpr.append(tpr[-1])\n","                fpr.append(fpr[-1] + 1 / total_negatives)\n","\n","        plt.figure()\n","        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\n","        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('Receiver Operating Characteristic (ROC) Curve with AUC = {:.2f}'.format(auc(fpr, tpr)))\n","        plt.legend(loc=\"lower right\")\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T09:40:08.645128Z","iopub.status.busy":"2024-01-08T09:40:08.644415Z","iopub.status.idle":"2024-01-08T09:40:08.655423Z","shell.execute_reply":"2024-01-08T09:40:08.653970Z","shell.execute_reply.started":"2024-01-08T09:40:08.645086Z"},"trusted":true},"outputs":[],"source":["def cbir_system_using_color_histogram(dataset_path, query_images_path, bin_size, features_file=\"features.pkl\"):\n","    cbir_system = CBIRSystem(dataset_path, features_file)\n","    cbir_system.load_images()\n","    cbir_system.extract_features(bin_size=bin_size)\n","    cbir_system.load_features()  # Load features before using search_similar_images\n","\n","    print(\"Number of images in the dataset: \", len(cbir_system.image_paths))\n","\n","    for query_image in os.listdir(query_images_path):\n","        query_image_path = os.path.join(query_images_path, query_image)\n","\n","        plt.imshow(cv2.cvtColor(cv2.imread(query_image_path), cv2.COLOR_BGR2RGB))\n","        plt.title(\"Query Image\")\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        result_images = cbir_system.search_similar_images(query_image_path, top_k=10, bin_size=bin_size)\n","        cbir_system.plot_results(query_image_path, result_images)\n","\n","    cbir_system.evaluate_performance(query_images_path, bin_size=bin_size)\n","    return cbir_system"]},{"cell_type":"markdown","metadata":{},"source":["Kaggle has only 19.5 GB of storage, so I called the function `delete_features_file` to clear the space for each new run.\n","\n","Then, a CBIR system using a color histogram was used with three different values of bins to compare them.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T09:40:10.182856Z","iopub.status.busy":"2024-01-08T09:40:10.182352Z","iopub.status.idle":"2024-01-08T09:40:40.297497Z","shell.execute_reply":"2024-01-08T09:40:40.296052Z","shell.execute_reply.started":"2024-01-08T09:40:10.182799Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["delete_features_file()\n","measure_memory_usage_and_time_spent(cbir_system_using_color_histogram, dataset_path, query_images_path, bin_size=8)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T08:31:59.743484Z","iopub.status.busy":"2024-01-08T08:31:59.741449Z","iopub.status.idle":"2024-01-08T08:34:47.160441Z","shell.execute_reply":"2024-01-08T08:34:47.159004Z","shell.execute_reply.started":"2024-01-08T08:31:59.743404Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["delete_features_file()\n","measure_memory_usage_and_time_spent(cbir_system_using_color_histogram, dataset_path, query_images_path, bin_size=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T08:06:04.108612Z","iopub.status.busy":"2024-01-08T08:06:04.108159Z","iopub.status.idle":"2024-01-08T08:14:30.027581Z","shell.execute_reply":"2024-01-08T08:14:30.026279Z","shell.execute_reply.started":"2024-01-08T08:06:04.108577Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["delete_features_file()\n","measure_memory_usage_and_time_spent(cbir_system_using_color_histogram, dataset_path, query_images_path, bin_size=150)"]},{"cell_type":"markdown","metadata":{},"source":["## Summary of Results\n","\n","**8 Bins:**\n","\n","- Average precision: 0.83\n","- Average Recall: 0.092\n","- Average F1 Score: 0.166\n","- Memory Usage: 37.60 MB\n","- Time Spent: 00:00:27\n","- AUC: 0.78\n","\n","**100 Bins:**\n","\n","- Average precision: 0.38\n","- Average Recall: 0.042\n","- Average F1 Score: 0.076\n","- Memory Usage: 3812.55 MB\n","- Time Spent: 00:02:46\n","- AUC: 0.67\n","\n","**150 Bins:**\n","\n","- Average precision: 0.33\n","- Average Recall: 0.037\n","- Average F1 Score: 0.066\n","- Memory Usage: 12808.48 MB\n","- Time Spent: 00:08:25\n","- AUC: 0.65\n","\n","## Comparison and Analysis\n","\n","**Effect of Bin Size:**\n","\n","The CBIR system with 8 bins shows higher precision, recall, and F1 score, suggesting that increasing the number of bins does not significantly improve performance in this scenario.\n","However, with 100 bins and 150 bins, the performance metrics dropped, indicating that a higher number of bins may result in a loss of discriminative information in the color histograms.\n","\n","**Memory Usage and Execution Time:**\n","\n","As the number of bins increases, both memory usage and execution time also increase significantly. This is expected because a higher number of bins leads to larger histograms and increased computational requirements during feature extraction.\n","\n","**AUC (Area Under the ROC Curve):**\n","\n","AUC values provide a measure of the system's ability to distinguish between positive and negative cases. In this case, a lower AUC with more bins suggests that the increased granularity in the color histograms might be introducing noise, making it harder to discriminate between relevant and irrelevant images.\n","In general, color histograms provide limited information about color distribution, overlooking spatial relationships, textures, and object shapes. They can be affected by lighting conditions and color variations, potentially affecting retrieval accuracy. Additionally, they may not accurately represent images with diverse color palettes and complex visual structures.\n","\n","**Why Fewer Bins Might Be Better?**\n","\n","- Robustness to Noise: Fewer bins provide more robust features and are less sensitive to minor color variations due to lighting or image compression.\n","- Computational Efficiency: Smaller histograms require less memory and processing time, leading to faster retrieval and lower resource consumption.\n","- Generalization: Coarser color representations can capture broader similarities, potentially enhancing the retrieval of images with slight color differences.\n"]},{"cell_type":"markdown","metadata":{},"source":["> ### **Task 3:** in this task you need to experiment with color moments as following:\n",">\n","> **3.1:** Implement the CBIR system using Color Moments (mean, standard deviation, and skewness) as an\n","> image representation. Use Euclidean as distant measure and assign equal weights to each moment.\n","> Compute precision, recall, F1 score, and time. Calculate the Area Under the Curve (AUC) to measure the\n","> overall performance across different threshold settings. Note that you need to compute these measures\n","> as an average of at least 10 different quires.\n"]},{"cell_type":"markdown","metadata":{},"source":["The `calculate_color_moments` method efficiently captures the color characteristics of an image by computing its mean, standard deviation, and skewness values. Utilizing the `np.mean` function, it calculates the average color values for each channel (red, green, and blue), providing insight into the central tendency of the color distribution. The computation of the standard deviation with `np.std` offers information about the spread or variation of color values around the mean, highlighting color diversity within the image. The method further reshapes the image into a 2D array and employs the `skew` function, likely from SciPy, to determine the skewness of each color channel. Skewness serves as a measure of asymmetry in the color distribution, indicating any bias towards higher or lower color values. Ultimately, the calculated mean, standard deviation, and skewness values are concatenated into a single feature vector using `np.concatenate`. This concise vector encapsulates the color moments of the image, providing a statistical summary of its color distribution. Such a representation is particularly valuable for image retrieval and various image processing tasks, offering a nuanced understanding of color characteristics beyond what is achievable with traditional color histograms.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T09:56:15.549830Z","iopub.status.busy":"2024-01-08T09:56:15.549324Z","iopub.status.idle":"2024-01-08T09:56:15.590754Z","shell.execute_reply":"2024-01-08T09:56:15.589175Z","shell.execute_reply.started":"2024-01-08T09:56:15.549779Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from scipy.stats import skew\n","from sklearn.metrics.pairwise import euclidean_distances\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_fscore_support, auc\n","import time\n","import psutil\n","\n","class CBIRSystem:\n","    def __init__(self, dataset_path):\n","        self.dataset_path = dataset_path\n","        self.image_paths = []\n","        self.features = []\n","\n","    def load_images(self):\n","        for root, dirs, files in os.walk(self.dataset_path):\n","            for file in files:\n","                if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    image_path = os.path.join(root, file)\n","                    self.image_paths.append(image_path)\n","\n","    def extract_features(self):\n","        for image_path in self.image_paths:\n","            img = cv2.imread(image_path)\n","            moments = self.calculate_color_moments(img)\n","            self.features.append(moments)\n","\n","    def calculate_color_moments(self, image):\n","        mean = np.mean(image, axis=(0, 1))\n","        std_dev = np.std(image, axis=(0, 1))\n","        skewness = skew(image.reshape(-1, 3), axis=0)\n","        return np.concatenate((mean, std_dev, skewness))\n","\n","    def search_similar_images(self, query_image_path, top_k=10):\n","        query_img = cv2.imread(query_image_path)\n","        query_moments = self.calculate_color_moments(query_img)\n","\n","        distances = euclidean_distances([query_moments], self.features).flatten()\n","        sorted_indices = np.argsort(distances)[:top_k]\n","\n","        return [\n","            (self.image_paths[i], os.path.basename(self.image_paths[i]))\n","            for i in sorted_indices\n","        ]\n","\n","    def plot_results(self, query_image_path, result_image_info):\n","        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","\n","        # Plot query image\n","        query_img = cv2.imread(query_image_path)\n","        axes[0, 0].imshow(cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB))\n","        axes[0, 0].set_title(\"Query Image\")\n","        axes[0, 0].axis(\"off\")\n","\n","        # Plot similar images\n","        for i, (result_image_path, result_image_name) in enumerate(result_image_info):\n","            img = cv2.imread(result_image_path)\n","            axes[i // 5, i % 5].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","            axes[i // 5, i % 5].set_title(result_image_name)\n","            axes[i // 5, i % 5].axis(\"off\")\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def calculate_euclidean_distances(self, query_image_path):\n","        query_img = cv2.imread(query_image_path)\n","        query_moments = self.calculate_color_moments(query_img)\n","\n","        distances = euclidean_distances([query_moments], self.features).flatten()\n","        return distances\n","\n","    def evaluate_performance(self, query_images_path):\n","        all_true_labels = []\n","        all_distances = []\n","        precision_list = []\n","        recall_list = []\n","        f1_score_list = []\n","        \n","        # Load ground truth\n","        ground_truth_labels = [1 if 'horses' in path and '710.jpg' <= path.split(os.path.sep)[-1] <= '799.jpg' else 0 for path in self.image_paths]\n","\n","        # Initialize arrays to store counts for each query\n","        TP_list = []\n","        FP_list = []\n","        TN_list = []\n","        FN_list = []\n","\n","        # Loop through each query image\n","        for query_image in os.listdir(query_images_path):\n","            # Initialize counts for the current query\n","            TP = 0\n","            FP = 0\n","            TN = 0\n","            FN = 0\n","\n","            query_image_path = os.path.join(query_images_path, query_image)\n","            distances = self.calculate_euclidean_distances(query_image_path)\n","            sorted_indices = np.argsort(distances)\n","\n","            true_labels = ground_truth_labels  # Use ground truth labels for evaluation\n","\n","            # Count the relevant images in the top-k results\n","            for i in range(10):\n","                if true_labels[sorted_indices[i]] == 1:\n","                    TP += 1\n","                else:\n","                    FP += 1\n","\n","            # Count the non-relevant images after the top-k results\n","            for i in range(10, len(self.image_paths)):\n","                if true_labels[sorted_indices[i]] == 0:\n","                    TN += 1\n","                else:\n","                    FN += 1\n","\n","            # Append counts for the current query to the arrays\n","            TP_list.append(TP)\n","            FP_list.append(FP)\n","            TN_list.append(TN)\n","            FN_list.append(FN)\n","\n","            # Store results for averaging\n","            all_true_labels.extend(true_labels)\n","            all_distances.extend(distances)\n","\n","            # Calculate metrics for the current query\n","            precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, distances <= distances[sorted_indices[9]], average='binary')\n","\n","            # Store metrics for averaging\n","            precision_list.append(precision)\n","            recall_list.append(recall)\n","            f1_score_list.append(f1_score)\n","\n","            print(\"Metrics for Query:\", query_image)\n","            print(\"Precision:\", precision)\n","            print(\"Recall:\", recall)\n","            print(\"F1 Score:\", f1_score)\n","\n","        # Calculate and print average Precision, Recall, and F1 Score\n","        avg_precision = sum(precision_list) / len(precision_list)\n","        avg_recall = sum(recall_list) / len(recall_list)\n","        avg_f1_score = sum(f1_score_list) / len(f1_score_list)\n","\n","        print(\"Average Precision:\", avg_precision)\n","        print(\"Average Recall:\", avg_recall)\n","        print(\"Average F1 Score:\", avg_f1_score)\n","        \n","\n","        # Manually calculate FPR and TPR\n","        sorted_indices_all = np.argsort(all_distances)\n","        total_positives = sum(all_true_labels)\n","        total_negatives = len(all_true_labels) - total_positives\n","\n","        fpr = [0]\n","        tpr = [0]\n","        for i in range(len(all_true_labels)):\n","            if all_true_labels[sorted_indices_all[i]] == 1:\n","                tpr.append(tpr[-1] + 1 / total_positives)\n","                fpr.append(fpr[-1])\n","            else:\n","                tpr.append(tpr[-1])\n","                fpr.append(fpr[-1] + 1 / total_negatives)\n","\n","        # Plot ROC curve\n","        plt.figure()\n","        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\n","        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('Receiver Operating Characteristic (ROC) Curve with AUC = {:.2f}'.format(auc(fpr, tpr)))\n","        plt.legend(loc=\"lower right\")\n","        plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T09:56:15.593360Z","iopub.status.busy":"2024-01-08T09:56:15.592911Z","iopub.status.idle":"2024-01-08T09:56:15.608326Z","shell.execute_reply":"2024-01-08T09:56:15.606989Z","shell.execute_reply.started":"2024-01-08T09:56:15.593325Z"},"trusted":true},"outputs":[],"source":["def cbir_system_using_color_moments(dataset_path, query_images_path):\n","    cbir_system = CBIRSystem(dataset_path)\n","    cbir_system.load_images()\n","    cbir_system.extract_features()\n","    print(\"Number of images in the dataset: \", len(cbir_system.image_paths))\n","\n","    for query_image in os.listdir(query_images_path):\n","        query_image_path = os.path.join(query_images_path, query_image)\n","\n","        plt.imshow(cv2.cvtColor(cv2.imread(query_image_path), cv2.COLOR_BGR2RGB))\n","        plt.title(\"Query Image\")\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        result_images = cbir_system.search_similar_images(query_image_path, top_k=10)\n","        cbir_system.plot_results(query_image_path, result_images)\n","\n","    # Evaluate performance\n","    cbir_system.evaluate_performance(query_images_path)\n","    return cbir_system"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T09:56:15.611074Z","iopub.status.busy":"2024-01-08T09:56:15.610134Z","iopub.status.idle":"2024-01-08T09:57:17.895236Z","shell.execute_reply":"2024-01-08T09:57:17.893788Z","shell.execute_reply.started":"2024-01-08T09:56:15.611026Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["measure_memory_usage_and_time_spent(cbir_system_using_color_moments, dataset_path, query_images_path)"]},{"cell_type":"markdown","metadata":{},"source":["> **3.2:** Same as task **3.1** but with different weights. You need to give a weigh relative to the important of the\n","> moment.\n"]},{"cell_type":"markdown","metadata":{},"source":["This task is like task 3.1 but with additional attribut `weights`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:23:56.755128Z","iopub.status.busy":"2024-01-08T17:23:56.754597Z","iopub.status.idle":"2024-01-08T17:23:56.796858Z","shell.execute_reply":"2024-01-08T17:23:56.795594Z","shell.execute_reply.started":"2024-01-08T17:23:56.755089Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from scipy.stats import skew\n","from sklearn.metrics.pairwise import euclidean_distances\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_fscore_support, auc\n","\n","class CBIRSystem:\n","    def __init__(self, dataset_path):\n","        self.dataset_path = dataset_path\n","        self.image_paths = []\n","        self.features = []\n","\n","    def load_images(self):\n","        for root, dirs, files in os.walk(self.dataset_path):\n","            for file in files:\n","                if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    image_path = os.path.join(root, file)\n","                    self.image_paths.append(image_path)\n","\n","    def extract_features(self, weights):\n","        for image_path in self.image_paths:\n","            img = cv2.imread(image_path)\n","            moments = self.calculate_color_moments(img, weights)\n","            self.features.append(moments)\n","\n","    def calculate_color_moments(self, image, weights):\n","        mean = np.mean(image, axis=(0, 1))\n","        std_dev = np.std(image, axis=(0, 1))\n","        skewness = skew(image.reshape(-1, 3), axis=0)\n","        \n","        weighted_moments = weights[0] * mean + weights[1] * std_dev + weights[2] * skewness\n","        return weighted_moments\n","\n","    def search_similar_images(self, query_image_path, weights, top_k=10):\n","        query_img = cv2.imread(query_image_path)\n","        query_moments = self.calculate_color_moments(query_img, weights)\n","\n","        distances = euclidean_distances([query_moments], self.features).flatten()\n","        sorted_indices = np.argsort(distances)[:top_k]\n","\n","        return [\n","            (self.image_paths[i], os.path.basename(self.image_paths[i]))\n","            for i in sorted_indices\n","        ]\n","\n","    def plot_results(self, query_image_path, result_image_info):\n","        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","\n","        # Plot query image\n","        query_img = cv2.imread(query_image_path)\n","        axes[0, 0].imshow(cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB))\n","        axes[0, 0].set_title(\"Query Image\")\n","        axes[0, 0].axis(\"off\")\n","\n","        # Plot similar images\n","        for i, (result_image_path, result_image_name) in enumerate(result_image_info):\n","            img = cv2.imread(result_image_path)\n","            axes[i // 5, i % 5].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","            axes[i // 5, i % 5].set_title(result_image_name)\n","            axes[i // 5, i % 5].axis(\"off\")\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def calculate_euclidean_distances(self, query_image_path, weights):\n","        query_img = cv2.imread(query_image_path)\n","        query_moments = self.calculate_color_moments(query_img, weights)\n","\n","        distances = euclidean_distances([query_moments], self.features).flatten()\n","        return distances\n","\n","    def evaluate_performance(self, query_images_path, weights):\n","        all_true_labels = []\n","        all_distances = []\n","        precision_list = []\n","        recall_list = []\n","        f1_score_list = []\n","        \n","        # Load ground truth\n","        ground_truth_labels = [1 if 'horses' in path and '710.jpg' <= path.split(os.path.sep)[-1] <= '799.jpg' else 0 for path in self.image_paths]\n","\n","        # Initialize arrays to store counts for each query\n","        TP_list = []\n","        FP_list = []\n","        TN_list = []\n","        FN_list = []\n","\n","        # Loop through each query image\n","        for query_image in os.listdir(query_images_path):\n","            # Initialize counts for the current query\n","            TP = 0\n","            FP = 0\n","            TN = 0\n","            FN = 0\n","\n","            query_image_path = os.path.join(query_images_path, query_image)\n","            distances = self.calculate_euclidean_distances(query_image_path, weights)\n","            sorted_indices = np.argsort(distances)\n","\n","            true_labels = ground_truth_labels  # Use ground truth labels for evaluation\n","\n","            # Count the relevant images in the top-k results\n","            for i in range(10):\n","                if true_labels[sorted_indices[i]] == 1:\n","                    TP += 1\n","                else:\n","                    FP += 1\n","\n","            # Count the non-relevant images after the top-k results\n","            for i in range(10, len(self.image_paths)):\n","                if true_labels[sorted_indices[i]] == 0:\n","                    TN += 1\n","                else:\n","                    FN += 1\n","\n","            # Append counts for the current query to the arrays\n","            TP_list.append(TP)\n","            FP_list.append(FP)\n","            TN_list.append(TN)\n","            FN_list.append(FN)\n","\n","            # Store results for averaging\n","            all_true_labels.extend(true_labels)\n","            all_distances.extend(distances)\n","\n","            # Calculate metrics for the current query\n","            precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, distances <= distances[sorted_indices[9]], average='binary')\n","\n","            # Store metrics for averaging\n","            precision_list.append(precision)\n","            recall_list.append(recall)\n","            f1_score_list.append(f1_score)\n","\n","            print(\"Metrics for Query:\", query_image)\n","            print(\"Precision:\", precision)\n","            print(\"Recall:\", recall)\n","            print(\"F1 Score:\", f1_score)\n","\n","        # Calculate and print average Precision, Recall, and F1 Score\n","        avg_precision = sum(precision_list) / len(precision_list)\n","        avg_recall = sum(recall_list) / len(recall_list)\n","        avg_f1_score = sum(f1_score_list) / len(f1_score_list)\n","\n","        print(\"Average Precision:\", avg_precision)\n","        print(\"Average Recall:\", avg_recall)\n","        print(\"Average F1 Score:\", avg_f1_score)\n","        \n","\n","        # Manually calculate FPR and TPR\n","        sorted_indices_all = np.argsort(all_distances)\n","        total_positives = sum(all_true_labels)\n","        total_negatives = len(all_true_labels) - total_positives\n","\n","        fpr = [0]\n","        tpr = [0]\n","        for i in range(len(all_true_labels)):\n","            if all_true_labels[sorted_indices_all[i]] == 1:\n","                tpr.append(tpr[-1] + 1 / total_positives)\n","                fpr.append(fpr[-1])\n","            else:\n","                tpr.append(tpr[-1])\n","                fpr.append(fpr[-1] + 1 / total_negatives)\n","\n","        # Plot ROC curve\n","        plt.figure()\n","        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\n","        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('Receiver Operating Characteristic (ROC) Curve with AUC = {:.2f}'.format(auc(fpr, tpr)))\n","        plt.legend(loc=\"lower right\")\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Choosing weights:**\n","\n","In this case, we have three moments: mean, standard deviation, and skewness. The weights can be assigned based on the relative importance I want to give to each moment in this scenario.\n","\n","`w_mean`: 0.5 (high weight, indicating that the mean is a crucial measure of central tendency).\n","\n","`w_std_dev`: 0.3 (medium weight, representing the importance of color variation around the mean).\n","\n","`w_skewness`: 0.2 (lower weight, as skewness provides additional information but may be less critical than mean and standard deviation).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:23:57.896200Z","iopub.status.busy":"2024-01-08T17:23:57.895748Z","iopub.status.idle":"2024-01-08T17:23:57.906524Z","shell.execute_reply":"2024-01-08T17:23:57.904891Z","shell.execute_reply.started":"2024-01-08T17:23:57.896164Z"},"trusted":true},"outputs":[],"source":["def cbir_system_using_color_moments(dataset_path, query_images_path):\n","    # weights for mean, std_dev, and skewness\n","    weights = [0.5, 0.3, 0.2]\n","\n","    cbir_system = CBIRSystem(dataset_path)\n","    cbir_system.load_images()\n","    cbir_system.extract_features(weights)\n","    print(\"Number of images in the dataset: \", len(cbir_system.image_paths))\n","\n","    for query_image in os.listdir(query_images_path):\n","        query_image_path = os.path.join(query_images_path, query_image)\n","\n","        plt.imshow(cv2.cvtColor(cv2.imread(query_image_path), cv2.COLOR_BGR2RGB))\n","        plt.title(\"Query Image\")\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        result_images = cbir_system.search_similar_images(query_image_path, weights, top_k=10)\n","        cbir_system.plot_results(query_image_path, result_images)\n","\n","    # Evaluate performance\n","    cbir_system.evaluate_performance(query_images_path, weights)\n","    return cbir_system\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:23:57.946764Z","iopub.status.busy":"2024-01-08T17:23:57.945730Z","iopub.status.idle":"2024-01-08T17:24:56.250277Z","shell.execute_reply":"2024-01-08T17:24:56.249031Z","shell.execute_reply.started":"2024-01-08T17:23:57.946723Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["measure_memory_usage_and_time_spent(cbir_system_using_color_moments, dataset_path, query_images_path)"]},{"cell_type":"markdown","metadata":{},"source":["> **3.3:** Same as task **3.2** but with the addition of more Moments including Median, Mode, and Kurtosis.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T16:52:03.598597Z","iopub.status.busy":"2024-01-08T16:52:03.598140Z","iopub.status.idle":"2024-01-08T16:52:03.653425Z","shell.execute_reply":"2024-01-08T16:52:03.651707Z","shell.execute_reply.started":"2024-01-08T16:52:03.598565Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from scipy.stats import skew, kurtosis\n","from scipy import stats\n","from sklearn.metrics.pairwise import euclidean_distances\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_fscore_support, auc\n","\n","class CBIRSystem:\n","    def __init__(self, dataset_path):\n","        self.dataset_path = dataset_path\n","        self.image_paths = []\n","        self.features = []\n","\n","    def load_images(self):\n","        for root, dirs, files in os.walk(self.dataset_path):\n","            for file in files:\n","                if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    image_path = os.path.join(root, file)\n","                    self.image_paths.append(image_path)\n","\n","    def extract_features(self, weights):\n","        for image_path in self.image_paths:\n","            img = cv2.imread(image_path)\n","            moments = self.calculate_color_moments(img, weights)\n","            self.features.append(moments)\n","\n","    def calculate_color_moments(self, image, weights):\n","        mean = np.mean(image, axis=(0, 1))\n","        std_dev = np.std(image, axis=(0, 1))\n","        skewness = skew(image.reshape(-1, 3), axis=0)\n","\n","        # Calculate median, mode, and kurtosis\n","        median = np.median(image, axis=(0, 1))\n","        mode = stats.mode(image, axis=(0, 1)).mode  # Extract mode values\n","        kurtosis = stats.kurtosis(image.reshape(-1, 3), axis=0)\n","\n","        # Combine all moments with weights\n","        weighted_moments = weights[0] * mean + weights[1] * std_dev + weights[2] * skewness + \\\n","                             weights[3] * median + weights[4] * mode + weights[5] * kurtosis\n","        return weighted_moments\n","\n","    def search_similar_images(self, query_image_path, weights, top_k=10):\n","        query_img = cv2.imread(query_image_path)\n","        query_moments = self.calculate_color_moments(query_img, weights)\n","\n","        distances = euclidean_distances([query_moments], self.features).flatten()\n","        sorted_indices = np.argsort(distances)[:top_k]\n","\n","        return [\n","            (self.image_paths[i], os.path.basename(self.image_paths[i]))\n","            for i in sorted_indices\n","        ]\n","\n","    def plot_results(self, query_image_path, result_image_info):\n","        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","\n","        # Plot query image\n","        query_img = cv2.imread(query_image_path)\n","        axes[0, 0].imshow(cv2.cvtColor(query_img, cv2.COLOR_BGR2RGB))\n","        axes[0, 0].set_title(\"Query Image\")\n","        axes[0, 0].axis(\"off\")\n","\n","        # Plot similar images\n","        for i, (result_image_path, result_image_name) in enumerate(result_image_info):\n","            img = cv2.imread(result_image_path)\n","            axes[i // 5, i % 5].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n","            axes[i // 5, i % 5].set_title(result_image_name)\n","            axes[i // 5, i % 5].axis(\"off\")\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def calculate_euclidean_distances(self, query_image_path, weights):\n","        query_img = cv2.imread(query_image_path)\n","        query_moments = self.calculate_color_moments(query_img, weights)\n","\n","        distances = euclidean_distances([query_moments], self.features).flatten()\n","        return distances\n","\n","    def evaluate_performance(self, query_images_path, weights):\n","        all_true_labels = []\n","        all_distances = []\n","        precision_list = []\n","        recall_list = []\n","        f1_score_list = []\n","        \n","        # Load ground truth\n","        ground_truth_labels = [1 if 'horses' in path and '710.jpg' <= path.split(os.path.sep)[-1] <= '799.jpg' else 0 for path in self.image_paths]\n","\n","        # Initialize arrays to store counts for each query\n","        TP_list = []\n","        FP_list = []\n","        TN_list = []\n","        FN_list = []\n","\n","        # Loop through each query image\n","        for query_image in os.listdir(query_images_path):\n","            # Initialize counts for the current query\n","            TP = 0\n","            FP = 0\n","            TN = 0\n","            FN = 0\n","\n","            query_image_path = os.path.join(query_images_path, query_image)\n","            distances = self.calculate_euclidean_distances(query_image_path, weights)\n","            sorted_indices = np.argsort(distances)\n","\n","            true_labels = ground_truth_labels  # Use ground truth labels for evaluation\n","\n","            # Count the relevant images in the top-k results\n","            for i in range(10):\n","                if true_labels[sorted_indices[i]] == 1:\n","                    TP += 1\n","                else:\n","                    FP += 1\n","\n","            # Count the non-relevant images after the top-k results\n","            for i in range(10, len(self.image_paths)):\n","                if true_labels[sorted_indices[i]] == 0:\n","                    TN += 1\n","                else:\n","                    FN += 1\n","\n","            # Append counts for the current query to the arrays\n","            TP_list.append(TP)\n","            FP_list.append(FP)\n","            TN_list.append(TN)\n","            FN_list.append(FN)\n","\n","            # Store results for averaging\n","            all_true_labels.extend(true_labels)\n","            all_distances.extend(distances)\n","\n","            # Calculate metrics for the current query\n","            precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, distances <= distances[sorted_indices[9]], average='binary')\n","\n","            # Store metrics for averaging\n","            precision_list.append(precision)\n","            recall_list.append(recall)\n","            f1_score_list.append(f1_score)\n","\n","            print(\"Metrics for Query:\", query_image)\n","            print(\"Precision:\", precision)\n","            print(\"Recall:\", recall)\n","            print(\"F1 Score:\", f1_score)\n","\n","        # Calculate and print average Precision, Recall, and F1 Score\n","        avg_precision = sum(precision_list) / len(precision_list)\n","        avg_recall = sum(recall_list) / len(recall_list)\n","        avg_f1_score = sum(f1_score_list) / len(f1_score_list)\n","\n","        print(\"Average Precision:\", avg_precision)\n","        print(\"Average Recall:\", avg_recall)\n","        print(\"Average F1 Score:\", avg_f1_score)\n","        \n","\n","        # Manually calculate FPR and TPR\n","        sorted_indices_all = np.argsort(all_distances)\n","        total_positives = sum(all_true_labels)\n","        total_negatives = len(all_true_labels) - total_positives\n","\n","        fpr = [0]\n","        tpr = [0]\n","        for i in range(len(all_true_labels)):\n","            if all_true_labels[sorted_indices_all[i]] == 1:\n","                tpr.append(tpr[-1] + 1 / total_positives)\n","                fpr.append(fpr[-1])\n","            else:\n","                tpr.append(tpr[-1])\n","                fpr.append(fpr[-1] + 1 / total_negatives)\n","\n","        # Plot ROC curve\n","        plt.figure()\n","        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\n","        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('Receiver Operating Characteristic (ROC) Curve with AUC = {:.2f}'.format(auc(fpr, tpr)))\n","        plt.legend(loc=\"lower right\")\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Choosing weights:**\n","\n","`Mean and Standard Deviation`: Given relatively higher weights of 0.25 and 0.2, respectively, mean and standard deviation continue to play significant roles in capturing the central tendency and variation in color distribution.\n","\n","`Skewness`: While skewness retains importance, its weight has been adjusted to 0.15, reflecting its role in characterizing the asymmetry of the color distribution.\n","\n","`Median`: The inclusion of the median with a weight of 0.15 indicates the importance of capturing central tendency, especially in the presence of outliers.\n","\n","`Mode and Kurtosis`: These features are assigned weights of 0.1 and 0.15, respectively, suggesting their roles in representing specific characteristics of the color distribution.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:00:40.303813Z","iopub.status.busy":"2024-01-08T17:00:40.303423Z","iopub.status.idle":"2024-01-08T17:00:40.314415Z","shell.execute_reply":"2024-01-08T17:00:40.313088Z","shell.execute_reply.started":"2024-01-08T17:00:40.303782Z"},"trusted":true},"outputs":[],"source":["def cbir_system_using_color_moments(dataset_path, query_images_path):\n","    # weights for mean, std_dev, skewness, median, mode, and kurtosis\n","    weights = [0.25, 0.2, 0.15, 0.15, 0.1, 0.15]\n","\n","    cbir_system = CBIRSystem(dataset_path)\n","    cbir_system.load_images()\n","    cbir_system.extract_features(weights)\n","    print(\"Number of images in the dataset: \", len(cbir_system.image_paths))\n","\n","    for query_image in os.listdir(query_images_path):\n","        query_image_path = os.path.join(query_images_path, query_image)\n","\n","        plt.imshow(cv2.cvtColor(cv2.imread(query_image_path), cv2.COLOR_BGR2RGB))\n","        plt.title(\"Query Image\")\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        result_images = cbir_system.search_similar_images(query_image_path, weights, top_k=10)\n","        cbir_system.plot_results(query_image_path, result_images)\n","\n","    # Evaluate performance\n","    cbir_system.evaluate_performance(query_images_path, weights)\n","    return cbir_system\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T17:00:40.334854Z","iopub.status.busy":"2024-01-08T17:00:40.334434Z","iopub.status.idle":"2024-01-08T17:02:17.072617Z","shell.execute_reply":"2024-01-08T17:02:17.071315Z","shell.execute_reply.started":"2024-01-08T17:00:40.334823Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["measure_memory_usage_and_time_spent(cbir_system_using_color_moments, dataset_path, query_images_path)"]},{"cell_type":"markdown","metadata":{},"source":["## Summary of Results\n","\n","**Case 1 with Mean, Std_Dev, and Skewness**:\n","\n","- Average precision: 0.71\n","- Average Recall: 0.0789\n","- Average F1 Score: 0.142\n","- Memory Usage: 49.57 MB\n","- Time Spent: 00:00:58\n","- AUC: 0.77\n","\n","**Case 2 with Std_Dev, Skewness, Median, Mode, and Kurtosis**:\n","\n","- Average precision: 0.67\n","- Average Recall: 0.0744\n","- Average F1 Score: 0.134\n","- Memory Usage: 30.48 MB\n","- Time Spent: 00:01:36\n","- AUC: 0.78\n","\n","## Analysis\n","\n","**Case 1**:\n","\n","The higher weight assigned to the mean (0.5) suggests a strong emphasis on the central tendency of the color distribution.\n","The overall performance metrics, including average precision, recall, and F1 score, indicate reasonable retrieval results.\n","\n","**Case 2**:\n","\n","The weights are more evenly distributed across different color moments.\n","The inclusion of additional moments (median, mode, and kurtosis) allows for a more comprehensive characterization of the color distribution.\n","Despite a slightly lower average precision, the system achieves a comparable AUC, indicating a good balance between precision and recall.\n","\n","## Conclusion\n","\n","Both cases gave good results with low memory usage, but with a lower time for case 1 and a slightly higher AUC for case 2.\n","\n","CBIR with color moments outperforms CBIR with color histograms due to their distinct characteristics and information capture. Histograms provide a global representation of color distribution but lack spatial information and discriminative power. Color moments provide a more detailed representation of color patterns, are more sensitive to spatial arrangement, provide higher-level information, and are flexible. The choice between these methods depends on the image's nature, retrieval system goals, and dataset characteristics.\n"]},{"cell_type":"markdown","metadata":{},"source":["> ### **Task 4:**\n",">\n","> Try to improve the performance of the CBIR system using other image representation techniques.\n","> "]},{"cell_type":"markdown","metadata":{},"source":["In this part, I tested several algorithms and the HOG algorithm gave me the best results. The algorithm calculates Histogram of Oriented Gradients (`HOG`) features for an image, which is a feature descriptor commonly used in computer vision and image processing for object detection and recognition. The algorithm involves input, calculation, and return of HOG features. The key parameters of the `hog` function include orientations, cell size, block size, and visualization.\n","\n","The HOG features can be used as a descriptor for the image, representing the spatial distribution of gradient orientations. They are particularly useful for representing the structure and texture of objects in an image. The algorithm's parameters, such as orientations, cell size, and block size, can be adjusted based on the specific requirements of the application.\n","\n","The HOG algorithm's implementation includes gradient computation, cellular grid, histogram creation, block normalization, and feature vector. The algorithm calculates the horizontal and vertical gradients for each pixel in the image using a derivative filter, obtaining the gradient magnitude and orientation for each pixel. It then divides the image into small, overlapping cells, creates a histogram of gradient orientations for each cell, and normalizes the histograms within each block to reduce the effect of lighting variations and enhance contrast.\n","\n","The code implementation calls the `feature.hog` function from a library `presumably scikit-image` to compute HOG features. The parameters include orientations, cell size, block size, visualization, and the return of the HOG feature vector.\n","\n","HOG features are robust to lighting variations and small changes in object position, capturing shape information effectively, making them useful for object detection and recognition tasks. However, parameter choices can impact performance and should be tuned for specific applications.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T10:03:40.336957Z","iopub.status.busy":"2024-01-08T10:03:40.336032Z","iopub.status.idle":"2024-01-08T10:03:40.374957Z","shell.execute_reply":"2024-01-08T10:03:40.373460Z","shell.execute_reply.started":"2024-01-08T10:03:40.336910Z"},"trusted":true},"outputs":[],"source":["import os\n","import cv2\n","import numpy as np\n","from skimage import feature\n","from sklearn.metrics.pairwise import euclidean_distances\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_fscore_support, auc\n","\n","class CBIRSystemHOG:\n","    def __init__(self, dataset_path):\n","        self.dataset_path = dataset_path\n","        self.image_paths = []\n","        self.features = []\n","\n","    def load_images(self):\n","        for root, dirs, files in os.walk(self.dataset_path):\n","            for file in files:\n","                if file.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n","                    image_path = os.path.join(root, file)\n","                    self.image_paths.append(image_path)\n","\n","    def extract_features(self):\n","        for image_path in self.image_paths:\n","            img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","            features = self.calculate_hog_features(img)\n","            self.features.append(features)\n","\n","    def calculate_hog_features(self, image):\n","        # Calculate HOG features\n","        hog_features, _ = feature.hog(image, orientations=8, pixels_per_cell=(16, 16),\n","                                      cells_per_block=(1, 1), block_norm=\"L2-Hys\", visualize=True)\n","\n","        return hog_features\n","\n","    def search_similar_images(self, query_image_path, top_k=10):\n","        query_img = cv2.imread(query_image_path, cv2.IMREAD_GRAYSCALE)\n","        query_features = self.calculate_hog_features(query_img)\n","\n","        distances = euclidean_distances([query_features], self.features).flatten()\n","        sorted_indices = np.argsort(distances)[:top_k]\n","\n","        return [\n","            (self.image_paths[i], os.path.basename(self.image_paths[i]))\n","            for i in sorted_indices\n","        ]\n","\n","    def plot_results(self, query_image_path, result_image_info):\n","        fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n","\n","        # Plot query image\n","        query_img = cv2.imread(query_image_path, cv2.IMREAD_GRAYSCALE)\n","        axes[0, 0].imshow(query_img, cmap='gray')\n","        axes[0, 0].set_title(\"Query Image\")\n","        axes[0, 0].axis(\"off\")\n","\n","        # Plot similar images\n","        for i, (result_image_path, result_image_name) in enumerate(result_image_info):\n","            img = cv2.imread(result_image_path, cv2.IMREAD_GRAYSCALE)\n","            axes[i // 5, i % 5].imshow(img, cmap='gray')\n","            axes[i // 5, i % 5].set_title(result_image_name)\n","            axes[i // 5, i % 5].axis(\"off\")\n","\n","        plt.tight_layout()\n","        plt.show()\n","\n","    def calculate_euclidean_distances(self, query_image_path):\n","        query_img = cv2.imread(query_image_path, cv2.IMREAD_GRAYSCALE)\n","        query_features = self.calculate_hog_features(query_img)\n","\n","        distances = euclidean_distances([query_features], self.features).flatten()\n","        return distances\n","    \n","    def evaluate_performance(self, query_images_path):\n","        all_true_labels = []\n","        all_distances = []\n","        precision_list = []\n","        recall_list = []\n","        f1_score_list = []\n","        \n","        # Load ground truth\n","        ground_truth_labels = [1 if 'horses' in path and '710.jpg' <= path.split(os.path.sep)[-1] <= '799.jpg' else 0 for path in self.image_paths]\n","\n","        # Initialize arrays to store counts for each query\n","        TP_list = []\n","        FP_list = []\n","        TN_list = []\n","        FN_list = []\n","\n","        # Loop through each query image\n","        for query_image in os.listdir(query_images_path):\n","            # Initialize counts for the current query\n","            TP = 0\n","            FP = 0\n","            TN = 0\n","            FN = 0\n","\n","            query_image_path = os.path.join(query_images_path, query_image)\n","            distances = self.calculate_euclidean_distances(query_image_path)\n","            sorted_indices = np.argsort(distances)\n","\n","            true_labels = ground_truth_labels  # Use ground truth labels for evaluation\n","\n","            # Count the relevant images in the top-k results\n","            for i in range(10):\n","                if true_labels[sorted_indices[i]] == 1:\n","                    TP += 1\n","                else:\n","                    FP += 1\n","\n","            # Count the non-relevant images after the top-k results\n","            for i in range(10, len(self.image_paths)):\n","                if true_labels[sorted_indices[i]] == 0:\n","                    TN += 1\n","                else:\n","                    FN += 1\n","\n","            # Append counts for the current query to the arrays\n","            TP_list.append(TP)\n","            FP_list.append(FP)\n","            TN_list.append(TN)\n","            FN_list.append(FN)\n","\n","            # Store results for averaging\n","            all_true_labels.extend(true_labels)\n","            all_distances.extend(distances)\n","\n","            # Calculate metrics for the current query\n","            precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, distances <= distances[sorted_indices[9]], average='binary')\n","\n","            # Store metrics for averaging\n","            precision_list.append(precision)\n","            recall_list.append(recall)\n","            f1_score_list.append(f1_score)\n","\n","            print(\"Metrics for Query:\", query_image)\n","            print(\"Precision:\", precision)\n","            print(\"Recall:\", recall)\n","            print(\"F1 Score:\", f1_score)\n","\n","        # Calculate and print average Precision, Recall, and F1 Score\n","        avg_precision = sum(precision_list) / len(precision_list)\n","        avg_recall = sum(recall_list) / len(recall_list)\n","        avg_f1_score = sum(f1_score_list) / len(f1_score_list)\n","\n","        print(\"Average Precision:\", avg_precision)\n","        print(\"Average Recall:\", avg_recall)\n","        print(\"Average F1 Score:\", avg_f1_score)\n","        \n","        # Manually calculate FPR and TPR\n","        sorted_indices_all = np.argsort(all_distances)\n","        total_positives = sum(all_true_labels)\n","        total_negatives = len(all_true_labels) - total_positives\n","\n","        fpr = [0]\n","        tpr = [0]\n","        for i in range(len(all_true_labels)):\n","            if all_true_labels[sorted_indices_all[i]] == 1:\n","                tpr.append(tpr[-1] + 1 / total_positives)\n","                fpr.append(fpr[-1])\n","            else:\n","                tpr.append(tpr[-1])\n","                fpr.append(fpr[-1] + 1 / total_negatives)\n","\n","        # Plot ROC curve\n","        plt.figure()\n","        plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve')\n","        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","        plt.xlabel('False Positive Rate')\n","        plt.ylabel('True Positive Rate')\n","        plt.title('Receiver Operating Characteristic (ROC) Curve with AUC = {:.2f}'.format(auc(fpr, tpr)))\n","        plt.legend(loc=\"lower right\")\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["All images were converted into `Grayscale` as they have a single channel representing intensity, making them computationally more efficient. They provide a uniform representation, focusing on structural details based on intensity variations. Grayscale images also reduce noise by eliminating the complexity of color information. They are effective at capturing object shapes and structures based on local gradient information. They are invariant to color changes, providing a consistent representation across diverse images. Grayscale images align with the HOG implementation, making them more effective for retrieval goals.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T10:03:40.472694Z","iopub.status.busy":"2024-01-08T10:03:40.472188Z","iopub.status.idle":"2024-01-08T10:03:40.484120Z","shell.execute_reply":"2024-01-08T10:03:40.482046Z","shell.execute_reply.started":"2024-01-08T10:03:40.472654Z"},"trusted":true},"outputs":[],"source":["def cbir_system_using_hog_features(dataset_path, query_images_path):\n","    cbir_system_hog = CBIRSystemHOG(dataset_path)\n","    cbir_system_hog.load_images()\n","    cbir_system_hog.extract_features()\n","    print(\"Number of images in the dataset: \", len(cbir_system_hog.image_paths))\n","\n","    for query_image in os.listdir(query_images_path):\n","        query_image_path = os.path.join(query_images_path, query_image)\n","\n","        plt.imshow(cv2.imread(query_image_path, cv2.IMREAD_GRAYSCALE), cmap='gray')\n","        plt.title(\"Query Image\")\n","        plt.axis(\"off\")\n","        plt.show()\n","\n","        result_images = cbir_system_hog.search_similar_images(query_image_path, top_k=10)\n","        cbir_system_hog.plot_results(query_image_path, result_images)\n","\n","    # Evaluate performance\n","    cbir_system_hog.evaluate_performance(query_images_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-01-08T10:03:41.978831Z","iopub.status.busy":"2024-01-08T10:03:41.978339Z","iopub.status.idle":"2024-01-08T10:06:45.142429Z","shell.execute_reply":"2024-01-08T10:06:45.141270Z","shell.execute_reply.started":"2024-01-08T10:03:41.978777Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["measure_memory_usage_and_time_spent(cbir_system_using_hog_features, dataset_path, query_images_path)"]},{"cell_type":"markdown","metadata":{},"source":["## Discussion\n","\n","HOG (Histogram of Oriented Gradients) may outperform Content-Based Image Retrieval (CBIR) with color histograms and color moments in certain scenarios due to its ability to capture shape and edge information, robustness to lighting and color variations, and discriminative power for specific tasks. HOG features capture the shapes, contours, and textures of an image through the distribution of oriented gradients, providing spatial information about the image. They are also relatively robust to lighting variations and color changes, making them effective for comparing images that might differ in brightness or overall color tone but share similar shapes and textures.\n","\n","However, HOG features can be computationally more expensive to calculate compared to color histograms and color moments, especially when dealing with large datasets or real-time applications. They might not be the best choice for tasks where color is the primary distinguishing factor, such as identifying specific color variations in textiles or artwork.\n","\n","The best choice for your CBIR system depends on your specific needs and dataset characteristics. HOG features may be a powerful option for retrieving images based on shapes, textures, and object recognition, but color-based features might be more suitable for tasks where color plays a dominant role in retrieval goals. Experimentation and evaluation are key to finding the optimal feature representation for your CBIR system.\n"]},{"cell_type":"markdown","metadata":{},"source":["### Conclusion\n","\n","This study explores the role of color features in Content-Based Image Retrieval (CBIR) tasks, focusing on the development and evaluation of a CBIR system. The study found that color histograms, color moments, and Histogram of Oriented Gradients (HOG) features were effective in capturing nuanced color relationships. However, color histograms showed moderate performance, while color moments showed improved retrieval results due to their statistical representation. HOG features, which focused on shape and texture, achieved the highest retrieval accuracy, highlighting the importance of structural information for comprehensive image similarity assessment. The study also highlighted the need for a balance between accuracy, memory usage, and processing time in each feature set, highlighting the need for careful consideration of task and resource constraints when selecting the optimal feature.\n","\n","### Effectiveness of Color Features\n","\n","Color features play a crucial role in image similarity, but their effectiveness in Content-Based Image Retrieval (CBIR) depends on several factors. Color moments are more effective than color histograms, as they incorporate statistical information for richer image representations. HOG features are the most effective, capturing object shapes and structures. The method's ability to operate on grayscale images, focus on local gradient information, and provide robustness to color variations contributes to its success. The choice of features depends on the purpose of the CBIR system, which includes shape and texture information for broader recognition tasks.\n","\n","### Insights and Recommendations\n","\n","The findings underscore the importance of directing future research efforts towards innovative approaches in content-based image retrieval (CBIR). Hybrid feature representations, combining both color and shape-based features, present a promising avenue for enhancing retrieval performance. Emphasizing the alignment of task-specific features with the goals and characteristics of CBIR systems is crucial, urging researchers to tailor feature selections to the specific requirements of their datasets. Exploring advanced feature extraction techniques, including the integration of convolutional neural networks (CNNs) for automatic feature extraction and learning, holds potential for further improvements. The prospect of personalized CBIR systems, adapting retrieval results based on user preferences and search history, introduces a user-centric dimension to enhance the overall search experience. Furthermore, domain-specific optimization strategies, encompassing feature selection, distance metrics, and learning algorithms, can be explored to tailor CBIR solutions for specific application domains such as medical imagery, art retrieval, or product search. These insights collectively provide a roadmap for future research endeavors aimed at advancing the effectiveness and applicability of CBIR systems.\n","\n","Finally, the study highlights the importance of feature representation in Computer-Based Image Retrieval (CBIR) tasks, highlighting the effectiveness of color-based and shape-based features. Color moments outperformed color histograms, while HOG features are effective for capturing intricate object details. The findings suggest that selecting features aligned with task requirements can improve efficiency and user-friendliness in CBIR systems. Understanding image characteristics, retrieval goals, and feature combinations is crucial for unlocking color's full potential in CBIR systems."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4274953,"sourceId":7359862,"sourceType":"datasetVersion"}],"dockerImageVersionId":30626,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"}},"nbformat":4,"nbformat_minor":4}
